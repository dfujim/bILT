\section{Theory \label{sec:theory}}

In a \gls{bnmr} experiment, the experimental observable is the $\beta$-decay asymmetry $A(t)$, measured by the normalized difference in counting rates between two opposing detectors $N_{i}$ (see \latin{e.g.},~\cite{1983-Ackermann-TCP-31-291, 2015-MacFarlane-SSNMR-68-1}).
Because of the parity violation in $\beta$-decay, $A(t)$ is proportional to nuclear spin-polarization $P(t)$ of the $\beta$-emitting probes.
That is,
\begin{equation}
   A(t) = A_{0} P(t),
\end{equation}
where the proportionality factor $A_{0}$ the depends on the details of the $\beta$-decay and the geometry of the experiment.
The quantity of real interest is $P(t)$, whose temporal behaviour is connected with the fundamental properties of the host material.

In order to introduce the \gls{bnmr} probes in a host material of interest, they are ion-implanted using a \gls{dc} beam for a finite period of time $\Delta$, typically on the order of few seconds.
On arrival, each \gls{bnmr} probe interacts with electromagnetic fields inside the host and relaxes (\latin{i.e.}, re-orients);
however, not all probes arrive at the same time, which must be accounted for.
Fortunately, this convolution with the beam can be solved analytically.
For probes arriving at time $t^{\prime}$ during a beam pulse of duration $\Delta$ and decaying at a later time $t$, the time dependence of $P(t)$ can be written as~\cite{2006-Salman-PRL-96-147601, 2015-MacFarlane-PRB-92-064409}
%
\begin{equation} \label{eq:polarization}
P(t) = P_0 \frac{ \displaystyle \int_{0}^{\xi} \exp \left [ -\left (\xi - t^{\prime} \right ) / \tau_{\beta} \right ] \, R \left (t,t^{\prime} \right ) \, \mathrm{d}t^{\prime} }{ \displaystyle \int_{0}^{\xi} \exp \left [ -t^{\prime} / \tau_{\beta} \right ] \, \mathrm{d}t^{\prime} },
%   P(t) = P_{0} \times
%   \begin{cases}
%      \frac{ \displaystyle \int_{0}^{t} \exp \left [ -\left (t - t^{\prime} \right ) / \tau_{\beta} \right ] \, R \left (t,t^{\prime} \right ) \, \mathrm{d}t^{\prime} }{ \displaystyle \int_{0}^{t} \exp \left [ -t^{\prime} / \tau_{\beta} \right ] \, \mathrm{d}t^{\prime} }, & t \leq \Delta \\
%      \frac{ \displaystyle \int_{0}^{\Delta} \exp \left [ - \left ( \Delta - t^{\prime} \right ) / \tau_{\beta} \right ] \,  R \left (t, t^{\prime} \right ) \, \mathrm{d}t^{\prime} }{ \displaystyle \int_{0}^{\Delta} \exp \left [ -t^{\prime} / \tau_{\beta} \right ] \, \mathrm{d} t^{\prime} }, & t > \Delta
%   \end{cases}
\end{equation}
%
where the integration limit $\xi = t$ if $t \leq \Delta$ and $\xi = \Delta$ otherwise,
$P_{0}$ is the degree of polarization at $t = 0$ (determined by the optical pumping of \ch{^{8}Li} prior to implantation),
and $R \left (t, t^{\prime} \right )$ is the relaxation function.

% I think this way of presenting it makes more sense with regards to describing the kernel matrix
The simplest form for $R \left (t, t^{\prime} \right )$ is a single decaying exponential: 
%
\begin{equation} \label{eq:slr-integral}
   R \left (t, t^{\prime} \right ) = \exp \left [-  \lambda \left ( t - t^{\prime} \right ) \right ],
\end{equation}
%
where $\lambda \equiv 1 / T_{1}$ is the \gls{slr} rate. Quite generally, however, $R \left (t, t^{\prime} \right )$ can be written as a distribution of decaying exponentials, with the distribution of the \gls{slr} rates described by some $p(\lambda)$.
This is quite analogous to \Cref{eq:signal-integral}.
In practice, spectra are often well-described (at the level of phenomenology) by a stretched exponential or \gls{kww} function~\cite{1854-Kohlrausch-AP-167-179, 1970-Williams-TFS-66-80, 1980-Lindsay-JCP-73-3348, 2006-Johnston-PRB-74-184430, 2016-Wu-SR-6-20506}.
In this case \Cref{eq:slr-integral} becomes:
%
\begin{equation} \label{eq:slr-stretched}
   R_\mathrm{KWW} \left ( t, t^{\prime} \right ) = \exp \left \{ - \left  [ \lambda \left ( t-t^{\prime} \right ) \right ]^{\beta} \right \},
\end{equation}
%
where $0 < \beta \leq 1$ is the stretching exponent.
This form of $R \left ( t, t^{\prime} \right )$ can arise when the \gls{kww} distribution is chosen for $p ( \lambda ) $~\cite{2006-Johnston-PRB-74-184430}.
The special cases of $\beta = 1/2$~\cite{1968-Tse-PRL-21-511, 1984-Stockmann-JNCS-66-501} or $\beta = 1/3$~\cite{1992-Bader-JPCM-4-4779} originate from \gls{slr} that is inhomogenously averaged in \gls{3d} or \gls{2d} limits.
The approach is often used in conventional solid-state \gls{nmr}~\cite{1995-Narayanan-JMRSA-112-58}, though chiefly as an expedient simplification of the multi-exponential magnetization transients, particularly when quadrupolar interactions are present~\cite{1995-McDowell-JMRSA-113-242}.
%In disordered or glassy materials like polymers a distribution of relaxation times is expected and a stretched exponential works works well~\cite{2014-McKenzie-JACS-136-7833, 2014-McGee-JPCS-551-012039, 2015-McKenzie-SM-11-1755, 2017-McKenzie-JCP-146-244903, 2018-McKenzie-SM-14-7324, 2019-Fujimoto-CM-31-9346}.
%While it may not be \latin{a priori} obvious why a relaxation transient appears stretched, the function has the following advantages as a phenomenological model:
%1) it accurately captures the characteristic $1/e$ relaxation time;
%and 2) it contains relatively few degrees of freedom (\latin{i.e.}, less than a biexponential model).

While the \gls{kww} distribution is successful at describing a great deal of relaxation phenomena, it is hardly the simplest distribution to work with.
In fact, it may be a little \emph{too} successful;
the high degree of correlation between $\lambda$ and $\beta$ allows \Cref{eq:slr-stretched} to do a good job in fitting relaxation spectra whose underlying $p ( \lambda )$ may be much simpler (\latin{e.g.}, a biexponential).
This ease of use may disguise the proper relaxation function and lead to an incorrect interpretation of the relaxation mechanism.
\textbf{More here?}

We now describe how the \gls{ilt} may be used to determine $p ( \lambda )$, revealing the underlying distribution of exponentials.
To start, we discretize $t$ and $\lambda$, such that \Cref{eq:slr-integral} may instead be written as
%\begin{equation}
%\label{eq:signal-discrete}
%y ( t_{i} ) = \sum_{j} P( \lambda_{j} ) \exp ( - \lambda_{j} t_{i}) ,
%\end{equation}
%
\begin{equation} \label{eq:slr-discrete}
   R \left (t_{i}, t^{\prime};\lambda_j \right ) = \exp \left [- \lambda_{j} \left ( t_{i} - t^{\prime} \right ) \right ] .
\end{equation}
%
With this transformation, we conveniently write $A(t)$ in matrix form:
%
\begin{equation} \label{eq:signal-matrix}
   \mathbf{A} = K \mathbf{p},
\end{equation}
%
where $\mathbf{p}$ is the vector containing the associated amplitudes, and $K$ is the kernel matrix:
%
\begin{equation}
   \label{eq:kernel}
   [K]_{ij} = A_{0} P(t_i;\lambda_j),
   %\exp \left ( - \lambda_{j} t_{i} \right )
\end{equation}
%
where $P(t_i;\lambda_j)$ is found by plugging \Cref{eq:slr-discrete} into \Cref{eq:polarization}.
%Of course, this can be easily re-cast as a vector of probability densities:
%\textbf{if the $\lambda_j$ are not evenly spaced, then this is not necessarily true. It may be better to instead leave this more vague and say it is easily cast to a normalized vector, without saying how.}
%
%\begin{equation}
%   \tilde{\mathbf{p}} = \frac{ \mathbf{p} }{ \sum_{i} [p]_{i} } .
%\end{equation}
%


To solve the discretized problem, the goal is to
%
\begin{equation}
   \label{eq:onnls}
   \arg \min_{\mathbf{P} \geq 0} || \Sigma \left ( K \mathbf{p} - \mathbf{y} \right ) ||^{2} ,
\end{equation}
%
where $\Sigma$ is a diagonal matrix of the reciprocal uncertainties, and $\mathbf{y}$ is the measured asymmetry. 

This is a \gls{nnls} problem.
While straightforward, success is not guaranteed.
Matrix inversion is unstable \dots
\textbf{
Need to mention the details of the data itself:
histograms (\latin{i.e.}, discrete points in time) with inhomogeneous (statistical) uncertainties.
One has the freedom to adjust the time resolution of the spectra, up to a maximum determined at the time of data acquisition.
For \ch{^{8}Li}, a typical minimum bin width is \SI{10}{\milli\second}, which is a factor of \num{\sim 121} smaller than its nuclear lifetime.
}


To overcome this potentially show-stopping detail, \emph{regularization} is often introduced to increase the stability of the inversion.
This is done through the addition of a penalty term with a scale factor $\alpha$.
With this modification, \Cref{eq:onnls} becomes
%
\begin{equation}
\label{eq:rnnls}
   \arg \min_{\mathbf{p} \geq 0} || \Sigma(K \mathbf{p} - \mathbf{y}) ||^{2} + \alpha || \mathbf{p} ||^{2} .
\end{equation}
%
\Cref{eq:rnnls} is commonly known as Tikhonov regularization~\cite{1995-Tikhonov-NMSIPP} or ridge regression.
Effectively, the added term $\alpha || \mathbf{p} ||^{2}$ seeks to prevent the coefficients in $\mathbf{p}$ from adopting large values without incurring a penalty.
Pragmatically, this has the effect that, when $\alpha = 0$, $\mathbf{p}$ is most likely to contain elements close to 1 (\latin{i.e.}, giving a ``spiky'' solution).
Conversely, when $\alpha = \infty$, it forces the elements of $\mathbf{p}$ to zero, as $\alpha || \mathbf{p} ||^{2}$ is the dominant contribution to \Cref{eq:rnnls}.
As $\alpha$ grows, it tends to broaden $\mathbf{p}(\lambda)$.
Within the bounds $0 \leq \alpha \leq \infty$, there exists an ``optimum'' for the scale or smoothing parameter;
however, it is not known \latin{a priori} and must be determined for meaningful results.




Note, however, that \Cref{eq:rnnls} is \emph{not} the only form that the penalty ``add on'' can take!
Similarly, note that $\alpha$ \emph{must} be ``tuned'' in order for meaningful result to be obtained.


\Cref{eq:rnnls} can be solved using \gls{nnls} optimization~\cite{1995-Lawson-SLSP}.



When it comes to finding the ``optimal'' $\alpha$, there are several approaches~\cite{2001-Kilmer-SIAMJMAA-22-1204}.
So, so many approaches~\cite{2011-Bauer-MCS-81-1795}, in fact!


One common approach is the so-called Butler-Reeds-Dawson method~\cite{1981-Butler-SIAMJNMA-18-381}, but this does not appear to apply here because our statistical errors/noise are intrinsically inhomogeneous across a spectrum.
\textbf{Is this the same as the ``the discrepancy principle''~\cite{1966-Morozon-DANSSR-167-510}?}
\textbf{It is possible this approach may work? See Eqs.~(7.9)--(7.10) on pp.~180--181 in~\cite{1998-Hansen-RDDIPP}.}
This is in contrast to, for example, an inversion-recovery NMR experiment, where the level of noise can be determined, in principle, by turning off the RF amplifier.



Independent of the minimization step, we can define the fit $\chi^{2}$ as:
\begin{equation} \label{eq:chi2}
   \chi^{2} = || \Sigma \left ( K \mathbf{p} - \mathbf{y} \right ) ||^{2} ,
\end{equation}
which is the familiar (weighted) sum of the squared residuals.
Similarly, the fit quality can be assessed using the reduced $\chi^{2}$:
\begin{equation} \label{eq:reduced-chi2}
   \tilde{\chi}^{2} = \frac{ \chi^{2} }{ \mathrm{dof} },
\end{equation}
where $\mathrm{dof} = N - n$, $N$ is the number of data points in the spectrum, and $n = 0$ is the number of free paramters.
\textbf{%
Is this really true?
I would have thought that, technically, we have at least $\mathrm{len} ( \mathbf{p} )$ ``fit parameters'' (possibly $\mathrm{len} ( \mathbf{p} ) + 1$ if you include $\alpha$).
Since we are always taking $n$ to be as large as possible so that $\mathbf{p} ( \lambda )$ approximates $p ( \lambda )$, its almost unavoidable that $n > N$.
The concept of a reduced $\chi^{2}$ may be murky in this context.
}




\gls{gcv} approach~\cite{1978-Craven-NM-31-377}:
Define
\begin{equation}
\label{eq:gcv}
   \min_{\alpha \geq 0} G ( \alpha ) = \frac{ || K \mathbf{p} - \mathbf{y} ||^{2} }{ \tau ( \alpha )^{2} } ,
\end{equation}
where
\begin{equation}
   \tau ( \alpha ) = \mathrm{Tr} \left [ \mathbf{I} - K \left ( K^{T} K + \alpha^{2} L^{T} L \right )^{-1} K^{T} \right ] .
\end{equation}
One can use an alternative method targeted at cases when $K$ is large~\cite{1997-Golub-JCGS-6-1}.
It is generally difficult to minimize $G ( \alpha )$ as it is generally flat.



Use L-curve analysis.
Visualize the size of the solution $|| \mathbf{p} ||$ vs.\ the residual $|| \Sigma ( K \mathbf{p} - \mathbf{y} ) ||$ as a parametric plot for different $\alpha$.
Find the $\alpha$ that is the ``corner'' of the L-curve.
This is equivalent to to maximizing $\alpha$ up to the point where its becomes detrimental to the ``fit''.
In some cases where a minimum in $|| \Sigma ( K \mathbf{p} - \mathbf{y} ) ||$ is pronounced, the $\alpha$ satisfying the ``corner'' condition is obvious.

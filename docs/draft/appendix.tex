\section{Miscellaneous \label{sec:miscellaneous}}

discussion of pitfalls~\cite{1983-Varah-SIAMJSSC-4-164}.

L-curve regularization~\cite{1992-Hansen-SIAMR-34-561, 1993-Hansen-SIAMJSC-14-1487}.
An earlier example~\cite{1990-Hansen-SIAMJSSC-11-503}?

Nice high-level article~\cite{1994-Craig-CP-8-648}.

bridge vs.\ lasso regularization methods~\cite{1998-Wenjiang-JCGS-7-397}.

\cite{2002-Venkataramanan-IEEETSP-50-1017}.

Review of \glspl{ilt} in noisy \gls{nmr} data~\cite{2013-Berman-CMRPA-42-72}.

Discussion of what exactly are \glspl{ilt}~\cite{2017-Fordham-DF-29-2}.

(see \latin{e.g.}, the classic textbook on the subject~\cite{1995-Lawson-SLSP}).

see also fairly recent multiexponential analysis review~\cite{1999-Istratov-RSI-70-1233} and numerical recipes~\cite{numerical-recipies}.

A recent, long review on regularization methods~\cite{2018-Benning-AN-27-1}.

Triangle method for finding the corner of the L-curve~\cite{2002-Castellanos-ANM-43-359}.

Example of using weights like we do~\cite{1999-Dunn-JMR-140-153}.

Some papers mention application to muons~\cite{1984-Honig-JCAM-10-113}.

People are still writing theses on regularization (see \latin{e.g.},~\cite{2011-OrozcoRodriguez-PhD}).

Paper with explicit re-casting of the equations being solved~\cite{2001-OLeary-SIAMJSC-23-1161} --- good to check against our implementation.

Another book by an authority on the topic~\cite{1998-Hansen-RDDIPP}.

\section{Implementation \label{sec:implementation}}

We use:
NumPy~\cite{2011-vanderWalt-CSE-13-22},
SciPy~\cite{2020-Virtanen-NM},
and Matplotlib~\cite{2007-Hunter-CSE-9-90}.
The \gls{nnls} optimization was performed via widely used Fortran subroutines~\cite{1995-Lawson-SLSP}.

On implementation of \Cref{eq:rnnls} using the \gls{nnls} algorithm, one quickly realizes that the algorithm is useful only for solving equations of the form $L\mathbf{p} = \mathbf{z}$. This is resolved by defining the following variables in block form: 
%
\begin{align}
    L &= 
    \left(\begin{array}{c}
        \Sigma K \\ \Gamma
    \end{array}\right) &
    \mathbf{z} &= \left(\begin{array}{c}
        \Sigma\mathbf{y} \\ \bm{0}
    \end{array}\right),
\end{align}
%
where $K$ is defined by \Cref{eq:kernel}, $\Sigma$ is the diagonal matrix of reciprocal uncertainties, and $\Gamma$ is the regularization matrix. In general, the choice of $\Gamma$ is general, however in this work we define $\Gamma \equiv \alpha I$. It is trivial to then show that 
%
\begin{equation}
||\mathbf{z}-L\mathbf{p}||^2 = ||\Sigma (\mathbf{y} - K\mathbf{p})||^2 + ||\Gamma\mathbf{p}||^2.
\end{equation}
%
